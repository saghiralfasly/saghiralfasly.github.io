<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Saghir Alfasly</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Saghir Alfasly</name>
              </p>
              <p>I am a Research Fellow at Mayo Clinic (Department of AI & Informatics), where I work on Medical Image Analysis and machine learning. 
              </p>
              <p>
               Previously, I've worked on Video Understanding, Human Activity Recognition, Video-to-Video Summarization, Multimodal Action Recognition, and 3D Video Synthesis.
              </p>
              <p style="text-align:center">
                <a href="mailto:saghiralfasly@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/Alfasly-CV-2022.pdf">CV</a> &nbsp/&nbsp
                <a href="data/SaghirAlfasly-bio.txt">Bio</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=BTFNIwYAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/saghir-alfasly-861a7013a/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/saghiralfasly/">Github</a> &nbsp/&nbsp
		<a href="https://www.youtube.com/channel/UCsgOjSJk119ts6Jb06GtNpA">Youtube</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My current interests include computer vision, machine learning, medical image analysis, and computational pathology.
              </p>
            </td>
          </tr>
        </tbody></table>
         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		 

          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/PathDino.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
<!--                 <img src='images/PathDino.jpg' width="160"> -->
              </div>
              <script type="text/javascript">
                function nerfsuper_start() {
                  document.getElementById('PathDino').style.opacity = "1";
                }

                function nerfsuper_stop() {
                  document.getElementById('PathDino').style.opacity = "0";
                }
                nerfsuper_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2311.08359">
                <papertitle>Rotation-Agnostic Image Representation Learning for Digital Pathology</papertitle>
              </a>
              <br>
	      <strong>Saghir Alfasly</strong>,
              <a>Abubakr Shafique</a>,
              <a >Peyman Nejat</a>, 
		<a> Jibran Khan </a>
	      <a >Areej Alsaafin</a>, 
	      <a >Ghazal Alabtah</a>, 
	      <a >H.R.Tizhoosh</a>, 
              <br>
              <em>Preprint</em>, 2023
              <br>
	      [<a href="https://rhazeslab.github.io/PathDino-Page/">Project webpage</a>]
              [<a href="https://huggingface.co/spaces/Saghir/PathDino">Demo</a>]
              [<a href="https://arxiv.org/abs/2311.08359">Paper</a>]
	      [<a href="https://arxiv.org/pdf/2303.00725.pdf">Supplementary</a>]
		[<a href="https://github.com/RhazesLab/PathDino">Code</a>]
		[<a href="https://portal.gdc.cancer.gov/repository?facetTab=files&filters=%7B%22op%22%3A%22and%22%2C%22content%22%3A%5B%7B%22op%22%3A%22in%22%2C%22content%22%3A%7B%22field%22%3A%22cases.project.program.name%22%2C%22value%22%3A%5B%22TCGA%22%5D%7D%7D%2C%7B%22op%22%3A%22in%22%2C%22content%22%3A%7B%22field%22%3A%22files.access%22%2C%22value%22%3A%5B%22open%22%5D%7D%7D%2C%7B%22op%22%3A%22in%22%2C%22content%22%3A%7B%22field%22%3A%22files.data_format%22%2C%22value%22%3A%5B%22svs%22%5D%7D%7D%2C%7B%22op%22%3A%22in%22%2C%22content%22%3A%7B%22field%22%3A%22files.experimental_strategy%22%2C%22value%22%3A%5B%22Diagnostic%20Slide%22%5D%7D%7D%5D%7D">Data</a>]
              <p></p>
              <p>
              In this work, we introduce a fast patch selection method (FPS) for efficient selection of representative patches while preserving spatial distribution. HistoRotate, is a 360‚àò rotation augmentation for training histopathology models, enhancing learning without compromising contextual information. PathDino, is a compact histopathology Transformer with five small vision transformer blocks and ‚âà9 million parameters.
              </p>
            </td>
          </tr>

		 	<tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/SDM.gif' width="160"></div>
                <img src='images/SDM.gif' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('SDM').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('SDM').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2311.09902">
                <papertitle>Selection of Distinct Morphologies to Divide & Conquer Gigapixel Pathology Images</papertitle>
              </a>
              <br>
              <a>Abubakr Shafique</a>,
              <strong>Saghir Alfasly</strong>,
	      <a >Areej Alsaafin</a>,
              <a >Peyman Nejat</a>, 
	      <a> Jibran Khan </a>
	      <a >H.R.Tizhoosh</a>,
              <br>
              <em>Preprint</em>, 2023
              <br>
              [<a href="https://arxiv.org/abs/2311.09902">Paper</a>]
	      [<a href="https://arxiv.org/pdf/2311.09902.pdf">Supplementary</a>]
              <p></p>
              <p>
              We propose SDM, a novel method for selecting diverse WSI patches, minimizing patch count while capturing all morphological variations. SDM outperforms the state-of-the-art, achieving high representativeness without needing parameter tuning.
              </p>
            </td>
          </tr>	
		 
	<tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/OSRE.gif' width="160"></div>
                <img src='images/OSRE.gif' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('OSRE').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('OSRE').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2303.00725.pdf">
                <papertitle>OSRE: Object-to-Spot Rotation Estimation for Bike Parking Assessment</papertitle>
              </a>
              <br>
	      <strong>Saghir Alfasly</strong>,
              <a>Zaid Al-Huda</a>,
              <a > Saifullahi Bello</a>, 
		<a> Ahmed Elazab </a>
	      <a >Jian Lu</a>, 
	      <a >Chen Xu</a>, 
              <br>
              <em>IEEE Transactions on Intelligent Transportation Systems</em>, 2023
              <br>
	      [<a href="https://saghiralfasly.github.io/OSRE-Project/">Project webpage</a>]
              [<a href="https://www.youtube.com/watch?v=W-ifWAbRwpM">Video</a>]
              [<a href="https://ieeexplore.ieee.org/document/10323254">Paper</a>]
	      [<a href="https://arxiv.org/pdf/2303.00725.pdf">Supplementary</a>]
		[<a href="https://github.com/saghiralfasly/OSRE">Code</a>]
		[<a href="https://github.com/saghiralfasly/SynthBRSet">Data</a>]
              <p></p>
              <p>
              We leveraged the power of 3D graphics and computer vision techniques to tackle a real-world problem, that we propose object-to-spot rotation estimation which is of particular significance for intelligent surveillance systems, bike-sharing systems, and smart cities. We introduced a rotation estimator (OSRE) that estimates a parked bike rotation with respect to its parking area.
              </p>
            </td>
          </tr>	


		 
		 
          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/SSTSA.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
<!--                 <img src='images/SSTSA.jpg' width="160"> -->
              </div>
              <script type="text/javascript">
                function nerfsuper_start() {
                  document.getElementById('SSTSA').style.opacity = "1";
                }

                function nerfsuper_stop() {
                  document.getElementById('SSTSA').style.opacity = "0";
                }
                nerfsuper_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9834306/">
                <papertitle>An Effective Video Transformer with Synchronized Spatiotemporal and Spatial Self-Attention for Action Recognition</papertitle>
              </a>
              <br>
	      <strong>Saghir Alfasly</strong>,
              <a>Charles K. Chui</a>,
              <a >Qingtang Jiang</a>, 
	      <a >Jian Lu</a>, 
	      <a >Chen Xu</a>, 
              <br>
              <em>IEEE Transactions on Neural Networks and Learning Systems</em>, 2022
              <br>
              [<a href="https://www.youtube.com/watch?v=E0PP6AtkbVE">Video</a>]
              [<a href="https://ieeexplore.ieee.org/document/9834306/">Paper</a>]
              <p></p>
              <p>
              We propose a new spatiotemporal attention scheme, termed synchronized spatiotemporal and spatial attention (SSTSA), which derives the spatiotemporal features with temporal and spatial multiheaded self-attention (MSA) modules.
              </p>
            </td>
          </tr>	 
		 
<tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/IMD.jpg' width="160"></div>
                <img src='images/IMD.jpg' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('IMD').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('IMD').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Alfasly_Learnable_Irrelevant_Modality_Dropout_for_Multimodal_Action_Recognition_on_Modality-Specific_CVPR_2022_paper.html">
                <papertitle>Learnable Irrelevant Modality Dropout for Multimodal Action Recognition on Modality-Specific Annotated Videos</papertitle>
              </a>
              <br>
	      <strong>Saghir Alfasly</strong>, 
	      <a >Jian Lu</a>, 
	      <a >Chen Xu</a>, 
	      <a >Yuru Zou</a>, 
              <br>
              <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022
              <br>             
              [<a href="https://openaccess.thecvf.com/content/CVPR2022/html/Alfasly_Learnable_Irrelevant_Modality_Dropout_for_Multimodal_Action_Recognition_on_Modality-Specific_CVPR_2022_paper.html">Paper</a>]
	      [<a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Alfasly_Learnable_Irrelevant_Modality_CVPR_2022_supplemental.pdf">Supp</a>]
              <p></p>
              <p>
              Multimodal learning for video understanding (Text, Audio, RGB, Motion). We present a multimodal learning approach that leverage several modalities and several on-the-shelf models for both audio and language understanding. We proposed Irrelevant Modality Dropout (IMD) that drops the irrelevant audio from further processing while fusing the relevant audio-visual data for better video understanding.
              </p>
            </td>
          </tr>
		 
	<tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/FastPicker.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
<!--                 <img src='images/FastPicker.jpg' width="160"> -->
              </div>
              <script type="text/javascript">
                function nerfsuper_start() {
                  document.getElementById('FastPicker').style.opacity = "1";
                }

                function nerfsuper_stop() {
                  document.getElementById('FastPicker').style.opacity = "0";
                }
                nerfsuper_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.sciencedirect.com/science/article/pii/S0925231222013121">
                <papertitle>FastPicker: Adaptive independent two-stage video-to-video summarization for efficient action recognition</papertitle>
              </a>
              <br>
	      <strong>Saghir Alfasly</strong>,
	      <a >Jian Lu</a>, 
	      <a >Chen Xu</a>, 
	      <a>Zaid Al-Hudad</a>    
              <a >Qingtang Jiang</a>, 
	      <a> ZhaosongLu </a>,
	      <a>Charles K. Chui</a>,
              <br>
              <em>Neurocomputing</em>, 2022
              <br>
              [<a href="https://www.youtube.com/watch?v=L7QK-asUdKA">Video</a>]
              [<a href="https://www.sciencedirect.com/science/article/pii/S0925231222013121">Paper</a>]
              <p></p>
              <p>
              This research study addresses the following question: To what extent can a fast independent adaptive algorithm select the most discriminative and representative frames to downsize huge video datasets while improving action recognition performance?
              </p>
            </td>
          </tr>	 

	 <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/weakly.png' width="160"></div>
                <img src='images/weakly.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('weakly').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('weakly').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/article/10.1007/s10489-022-04212-w?fbclid=IwAR31l1KV59p8ULSLZpBtxtau8dXP2-JpDXAC7LtLtgL2C0D_vxFrCi6lqSU">
                <papertitle>Weakly supervised pavement crack semantic segmentation based on multi-scale object localization and incremental annotation refinement</papertitle>
              </a>
              <br>
	      <a>Zaid Al-Hudad</a> 
  	      <a>Bo Peng</a>
	      <a>Riyadh Nazar Ali Algburi</a>,
	      <strong>Saghir Alfasly</strong>, 
	      <a>Tianrui Li </a>, 
              <br>
              <em>Applied Intelligence</em>, 2022
              <br>             
              [<a href="https://link.springer.com/article/10.1007/s10489-022-04212-w?fbclid=IwAR31l1KV59p8ULSLZpBtxtau8dXP2-JpDXAC7LtLtgL2C0D_vxFrCi6lqSU">Paper</a>]
              <p></p>
              <p>
              Weakly supervised pavement crack semantic segmentation based on multi-scale object localization and incremental annotation refinement.
              </p>
            </td>
          </tr>
		 
		 
		 
          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/mlsl.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
<!--                 <img src='images/mlsl.jpg' width="160"> -->
              </div>
              <script type="text/javascript">
                function nerfsuper_start() {
                  document.getElementById('mlsl').style.opacity = "1";
                }

                function nerfsuper_stop() {
                  document.getElementById('mlsl').style.opacity = "0";
                }
                nerfsuper_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/8879470/">
                <papertitle>Multi-Label-Based Similarity Learning for Vehicle Re-Identification</papertitle>
              </a>
              <br>
	      <strong>Saghir Alfasly</strong>,
              <a>Yongjian Hu</a>,
              <a>Haoliang Li</a>, 
	      <a>Tiancai Liang</a>, 
	      <a>Xiaofeng Jin</a>, 
	      <a>BeiBei Liu</a>,
	      <a>Qingli Zhao</a>,
              <br>
              <em>IEEE Access</em>, 2019
              <br>
              [<a href="https://ieeexplore.ieee.org/abstract/document/8879470/">Paper</a>]
              [<a href="https://youtu.be/grlwdaaYPX4 ">Video</a>]
              <p></p>
              <p>
              We propose a multi-label similarity learning framework for vehicle re-identification.
              </p>
            </td>
          </tr>	 
		 
<tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='VFL'>
                  <img src='images/VFL.gif' width="160"></div>
                <img src='images/VFL.gif' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('VFL').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('VFL').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/8803366">
                <papertitle>Variational Representation Learning for Vehicle Re-Identification</papertitle>
              </a>
              <br>
	      <strong>Saghir Alfasly</strong>, 
	      <a >Yongjian Hu</a>, 
	      <a >Tiancai Liang</a>, 
	      <a >Xiaofeng Jin</a>, 
		<a >Qingli Zhao</a>, 
		<a >Beibei Liu</a>, 
              <br>
              <em>IEEE International Conference on Image Processing</em>, 2019
              <br>             
              [<a href="https://ieeexplore.ieee.org/document/8803366">Paper</a>]
	      [<a href="https://github.com/saghiralfasly/VFL-Vehicle-Re-Id">Github</a>] 
              <p></p>
              <p>
              We propose variational Representation Learning for object Re-Identification. The proposed method has been evaluated on vehicle re-identification and person re-identification and face recognition.
              </p>
            </td>
          </tr>		 

		 
		 
          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
<!-- 		<div class="two" id='VFL'>
		  <img src='images/autozoom.jpg' width="160"></div>
                  <img src='images/autozoom.jpg' width="160"> -->
		    
                 <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/autozoom.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div> 
<!--                  <img src='images/autozoom.png' width="160">  -->
              </div>
              <script type="text/javascript">
                function nerfsuper_start() {
                  document.getElementById('autozoom').style.opacity = "1";
                }

                function nerfsuper_stop() {
                  document.getElementById('autozoom').style.opacity = "0";
                }
                nerfsuper_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/8781786">
                <papertitle>Auto-Zooming CNN-Based Framework for Real-Time Pedestrian Detection in Outdoor Surveillance Videos</papertitle>
              </a>
              <br>
	      <strong>Saghir Alfasly</strong>,
		<a>BeiBei Liu</a>,
              <a>Yongjian Hu</a>,
              <a>Yufei Wang</a>, 
	      <a>Chang-Tsun Li</a>, 
              <br>
              <em>IEEE Access</em>, 2019
              <br>
              [<a href="https://ieeexplore.ieee.org/abstract/document/8781786">Paper</a>]
              [<a href="https://github.com/saghiralfasly/Lightweight_Baseline_for_Pedestrian_Detector">Github</a>]
	      [<a href="https://youtu.be/neRkY86ZYuk">Video</a>]
              <p></p>
              <p>
              For small objects detection like pedestrians in the outdoor surveillance, we propose a fast, lightweight, and auto-zooming-based framework for small pedestrian detection. 
              </p>
            </td>
          </tr>	 
		 
	 
      </td>
    </tr>
  </table> <br>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Service</heading> <br> <br>
	      <strong>Reviewer</strong> <br>
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">IEEE Transactions on Image Processing</a> <br>
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">IEEE Transactions on Neural Networks and Learning Systems</a> <br>
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">IEEE Transactions on Circuits and Systems for Video Technology</a> <br>
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6979">IEEE Transactions on Intelligent Transportation Systems</a> <br>
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9078688">IEEE Transactions on Artificial Intelligence</a> <br>
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7274857">IEEE Transactions on Intelligent Vehicles</a> <br> 
<a href="https://www.sciencedirect.com/journal/pattern-recognition">Pattern Recognition</a> <br>
<a href="https://www.journals.elsevier.com/pattern-recognition-letters">Pattern Recognition Letters</a> <br>  
<a>CVPR 2023,</a> <a>CVPR 2024</a> <br>
<a>ICCV 2023</a> <br>
<a>ECCV 2024</a> <br>
<a>ACM MM</a> <br> 
<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6287639">IEEE Access</a> <br>  
            </td>
          </tr>
        </tbody></table> 
	<br>
	<br>
	
	
<!-- 	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Talk</heading> <br> <br>
		    <p >[Jan-2018] "Intelligent Video Analysis Solutions with Deep Learning", GRG Banking - Annual Training Programme </p> <br> 
            </td>
          </tr>
        </tbody></table> -->
	
<p> Source: jonbarron/website </p>
</body>

</html>
